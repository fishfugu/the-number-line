# This robots.txt file blocks all crawlers from accessing any pages
User-agent: *
Disallow: /

# later - switch it to simply: 
# Disallow: /restricted/